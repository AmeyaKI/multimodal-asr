{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN37qnpC9Jr+KlyqhJQlfEk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmeyaKI/multimodal-asr/blob/main/asr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nNb7i4IgpB5"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoProcessor, AutoModelForCTC\n",
        "import librosa\n",
        "import os"
      ],
      "metadata": {
        "id": "h4FJ4B2ktoX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# randomly selected audio\n",
        "sample_path = \"/content/drive/MyDrive/colab_notebooks/asr/sample.wav\""
      ],
      "metadata": {
        "id": "2sE67ZZYtMqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU name:\", torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "id": "KTbZsi_YtSPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = 'nvidia/parakeet-ctc-1.1b' # train custom model ?\n",
        "device = 'cuda'\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(MODEL)\n",
        "model = AutoModelForCTC.from_pretrained(MODEL, dtype='auto')\n",
        "\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "cDYtBYoLxYR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "waveform, sample_rate = librosa.load(\n",
        "    sample_path,\n",
        "    sr=None,\n",
        "    mono=True\n",
        ")"
      ],
      "metadata": {
        "id": "Qqonu0KCxZ_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# resample to 16,000 hz\n",
        "TARGET_SAMPLE_RATE = 16000\n",
        "if sample_rate != TARGET_SAMPLE_RATE:\n",
        "    waveform = librosa.resample(\n",
        "        waveform,\n",
        "        orig_sr = sample_rate,\n",
        "        target_sr = TARGET_SAMPLE_RATE\n",
        "    )\n",
        "# normalizing audio amp\n",
        "max_val = np.max(np.abs(waveform))\n",
        "if max_val > 0:\n",
        "    waveform /= max_val"
      ],
      "metadata": {
        "id": "a1YUQDadx81-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calc model inputs\n",
        "model_inputs = processor(\n",
        "    waveform,\n",
        "    sampling_rate = TARGET_SAMPLE_RATE,\n",
        "    return_tensors = 'pt'\n",
        ")\n",
        "model_inputs = model_inputs.to(\n",
        "    device = model.device,\n",
        "    dtype = model.dtype\n",
        ")"
      ],
      "metadata": {
        "id": "ELJHYuWp0jha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifying\n",
        "print(model_inputs.input_features.dtype)\n",
        "print(next(model.parameters()).dtype)"
      ],
      "metadata": {
        "id": "uljsyOQh7OqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# obtain logits\n",
        "with torch.no_grad():\n",
        "    # Ensure input_features are in bfloat16 to match model's dtype\n",
        "    model_inputs.input_features = model_inputs.input_features.to(torch.bfloat16)\n",
        "    logits = model(**model_inputs).logits\n",
        "    # (batch size, time steps, vocab size)"
      ],
      "metadata": {
        "id": "eBUEY9MyzkpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decode and trnaslate logits to predicted text\n",
        "predicted_ids = torch.argmax(logits, dim=-1)\n",
        "transcription = processor.batch_decode(predicted_ids)\n",
        "\n",
        "text = transcription[0]\n",
        "print(text)"
      ],
      "metadata": {
        "id": "6UtYkaUUyAJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy: Character Error Rate"
      ],
      "metadata": {
        "id": "aUDLp5319HzG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}